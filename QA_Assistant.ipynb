{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jDleLTWahALu",
    "outputId": "ba0568b1-8e3f-47d0-8fee-bee25e3c7d6f"
   },
   "outputs": [],
   "source": [
    "!pip install transformers faiss-cpu torch fitz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xneNOTexh7Nz",
    "outputId": "99963ef0-a4f7-4b3b-a98c-2d9e696906b9"
   },
   "outputs": [],
   "source": [
    "!pip uninstall pymupdf\n",
    "! pip install pymupdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MExFalwajmOi",
    "outputId": "b89b0988-ed88-4767-8dd2-e2e06424f6be"
   },
   "outputs": [],
   "source": [
    "# Install required libraries (make sure you run these in Colab first)\n",
    "!pip install transformers torch fitz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "465968f6bee043749dcac3ad24028698",
      "b16afbc1c8b5454fb248cff3c950e14e",
      "01304ac8140d4871b55d40aacb73e826",
      "93f95dc68f304734888357b601ffcf9b",
      "7d273cdd41f74ef3a8c12e281ceacf75",
      "0fb9a4094d494fdabfaab977cdc1b70b",
      "9805d5aa41f240738ea9623e05c9afc1",
      "cc6ad6cfa30c42cf899f37e227b8d9af",
      "a8522dd1c12e49bab2943ca1b57109ef",
      "5833523396364443a2979d36a2baf03a",
      "4b895550711c437eb438b8b010cc6872",
      "9bb3414987cc459e826ae981a677fd19",
      "ffe4737dda754c4fac454b318e461f0c",
      "39bc930972ba4fc28bec9c8c12e4f064",
      "476857ef891a4bbdb6cc0931556d813e",
      "912834de9d6043ca99a72ec6f349e6cb",
      "6264c3ebe17f442e9b139ed6d3ada493",
      "016d6f6a818e41eea9eb578d5ca27069",
      "c51ef5f1ffee45fb9ac2bd9afa88c31e",
      "685442d4e698402c9f9b3af70592aeb1",
      "9e6eb14f411847daa8b9054157373cf8",
      "438d0406639949eb8f49ad000497c7d7",
      "13b103ecc37546d591178c85b8b1146a",
      "4224512d75884479b39604ac93888840",
      "b0cc44b1d7eb44f9ab5be8a5d45c8412",
      "850399f2600147128d7cd617581dcc02",
      "ab04818121914b9f97bea0488640502b",
      "e9c51f1029a94b45bb41704b035f4926",
      "5c16fc496fda4eaab56ab09845feff1b",
      "d5828b4e04754aafa4af1b3da582f489",
      "a2222de5b297438b8aa2e8c6f0e80b22",
      "9e9809b74acd4d3389e31e25d852ac3a",
      "cc336d357f5d4c11a116ed231026675c",
      "d1f71d6db65d4ff793ccf91d7df572ba",
      "9ebad9c3180749eabba035f0195edef9",
      "3da97b0b85bc4115bcf2de691701bb90",
      "9bb772531967407781aaab15b6a3a873",
      "8c4219fae63746a19dc8b93e78013eb3",
      "6b157ad7c604415592bf1d695904702e",
      "c8ec36305999444d9dc7ac3e9fa7cac5",
      "96d43ca99e7c4825b96e9d068a0d2cf4",
      "2b6bc1180fd54486b45cbe5ae3190d04",
      "51aba9dedb8447fb996b9aef147a4c68",
      "bad3f7799ddb46289f965ec106547b4e",
      "4b337d5082c54db0b62dadce865f347b",
      "7b18838abcf14418b5e364b5c8d7bccc",
      "ebeda42bb4224b99b4cbff61bfd14b1a",
      "820f2b4e5a524fdbafa2000c883ae5cc",
      "7d750d99066547a78bbad9bcc8f1fa9a",
      "88732c8d5f72435c911727ea6ec12278",
      "7919096b138240e1b7e04a96ab972d13",
      "2b21655c63f841eda74f84a8cb3a71a7",
      "f0233bd364d448a4abe78441f4b03864",
      "522b35df8f044568878a2ac8cef4a4a4",
      "99be08a1667242c39c3a3dcede3cef90"
     ]
    },
    "id": "PXwoVHnEhFQD",
    "outputId": "891bd6d2-f482-4137-fe7d-935c505a282e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, GPT2LMHeadModel, GPT2Tokenizer\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import time\n",
    "import textwrap\n",
    "\n",
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "\n",
    "# Load GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set pad_token to eos_token\n",
    "\n",
    "# Extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    return \" \".join([page.get_text() for page in doc])\n",
    "\n",
    "# Break the full text into chunks to avoid token overflow\n",
    "def chunk_text(text, chunk_size=500):\n",
    "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# Generate an answer from a chunk using GPT-2\n",
    "def answer_question_with_qa_model(question, context_chunk):\n",
    "    try:\n",
    "        result = qa_pipeline(question=question, context=context_chunk)\n",
    "        return result['answer']\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Only return what's after \"Answer:\"\n",
    "    return generated_text.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "# Chatbot class that uses all the above\n",
    "class PDFChatbot:\n",
    "    def __init__(self, pdf_path):\n",
    "        print(\"ðŸ” Loading and processing PDF...\")\n",
    "        self.text_chunks = chunk_text(extract_text_from_pdf(pdf_path))\n",
    "        print(f\"âœ… Loaded {len(self.text_chunks)} chunks.\")\n",
    "\n",
    "    def chat(self, question):\n",
    "      best_response = \"\"\n",
    "      best_score = 0\n",
    "\n",
    "      for chunk in self.text_chunks:\n",
    "        response = answer_question_with_qa_model(question, chunk)\n",
    "        score = len(response.strip())\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_response = response\n",
    "\n",
    "      return best_response or \"Sorry, I couldn't find an answer in the PDF.\"\n",
    "\n",
    "\n",
    "# === Run the chatbot ===\n",
    "pdf_path = \"/content/Seizure Detection and Probability Prediction using Random Forests.pdf\"\n",
    "chatbot = PDFChatbot(pdf_path)\n",
    "\n",
    "def pretty_print_response(response):\n",
    "    print(\"\\nChatbot ðŸ¤–: \", end=\"\")\n",
    "    for char in response:\n",
    "        print(char, end=\"\", flush=True)\n",
    "        time.sleep(0.01)  # Typing effect\n",
    "    print(\"\\n\")  # Add a new line at the end for spacing\n",
    "\n",
    "print(\"\\nâœ¨ Chatbot is ready to answer your questions! Type 'exit', 'quit', or 'stop' to end.\\n\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"ðŸ§  You: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\", \"stop\"]:\n",
    "        print(\"Chatbot ðŸ¤–: Goodbye! ðŸ‘‹\")\n",
    "        break\n",
    "    response = chatbot.chat(user_input)\n",
    "\n",
    "    # Wrap long lines (optional)\n",
    "    wrapped_response = textwrap.fill(response, width=80)\n",
    "    pretty_print_response(wrapped_response)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
